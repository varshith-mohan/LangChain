{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "vV0QmsW-aOmx",
        "outputId": "79d9237d-8f1a-45c0-8803-a69b31197fbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
            "Collecting langchain-core<2.0.0,>=1.0.3 (from langchain_huggingface)\n",
            "  Downloading langchain_core-1.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (2.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.10.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_core-1.0.7-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m473.0/473.0 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_huggingface\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.0.7 langchain_huggingface-1.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "63ab41852d654a849556c679cc2a0fc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "qkWxIy6GaLsn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel"
      ],
      "metadata": {
        "id": "T9BnlQCBjHfg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTTEE1j5aEjk",
        "outputId": "c3d50c5e-11cd-42c6-990f-2d86b5cbcbab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This joke is funny because it combines our understanding of typical developer workflows with the reference to AI's ability, even in its simulated state, to have straightforward goals. \n",
            "\n",
            "Here's the breakdown:\n",
            "\n",
            "* **\"Unit tests passed, now just need to run this script...\"** -  This is a familiar line for programmers, it signifies they're close to deploying their code. It plays on the expectation of a simple script for deployment. \n",
            "* **\"Endless checklist\"** - This refers to the complex and extensive list of processes and steps that are usually involved with deploying any software, even to AI systems. This checklist is often daunting and can seem endless, especially when dealing with potentially sensitive nature of how AI is deployed. \n",
            "* **\"Developer goggles\"** - This is a humorous reference to how developers sometimes wear specialized mode which helps them focus on details. \n",
            "\n",
            "**Putting it together:**\n",
            "\n",
            "The joke satirizes the stressful, intricate, and slightly comical aspects of deploying any kind of software. The AI persona embodies the very user experience developers share.  By highlighting those complexities through the 'developer lens'  and mentioning test results, it creates a relatable and humorous moment. \n",
            "\n",
            "\n",
            "Let me know if you want to hear another joke! üòä \n",
            "\n"
          ]
        }
      ],
      "source": [
        "llm = HuggingFaceEndpoint(repo_id='google/gemma-2-2b-it', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Write a joke about {topic}',\n",
        "    input_variables=['topic'])\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Explain the following joke - {text}',\n",
        "    input_variables=['text']\n",
        ")\n",
        "\n",
        "chain = prompt1 | model | parser | prompt2 | model | parser\n",
        "\n",
        "print(chain.invoke({'topic':'AI in DevOps'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TKlmbw-mmZa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Runnable Parallel**"
      ],
      "metadata": {
        "id": "VYKIyPDvbV2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate a tweet about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate a Linkedin post about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id='google/gemma-2-2b-it', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "tweet_chain    = prompt1 | model | parser\n",
        "linkedin_chain = prompt2 | model | parser\n",
        "\n",
        "parallel_chain = RunnableParallel({\n",
        "    'tweet': tweet_chain,\n",
        "    'linkedin': linkedin_chain})\n",
        "\n",
        "\n",
        "result = parallel_chain.invoke({'topic':'Fitness'})\n",
        "\n",
        "print(result['tweet'])\n",
        "print(result['linkedin'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQKc7KNqi7k5",
        "outputId": "4be6990d-da45-4e5b-93d7-8d96ad774def"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are a few tweet options about fitness, varying in tone and focus:\n",
            "\n",
            "**Motivational:**\n",
            "\n",
            "* Breaking out my workout clothes and getting ready to crush it! üí™ Let's gooooooo! #fitness #motivation #healthylifestyle\n",
            "* Feeling strong and energized after an amazing workout. ‚ö°Ô∏è Nothing beats the feeling of accomplishment! #proudofmyself #fitnessjourney\n",
            "\n",
            "**Informative/Sharing an Achievement:**\n",
            "\n",
            "* Finally hit my personal best for squats today! üéâ Feeling so satisfied! #fitnessgains #gains #exercise \n",
            "* Learning something new about fitness today - trying out foam rolling for the first time! Feeling the difference already. ü§∏‚Äç‚ôÄÔ∏è \n",
            "\n",
            "**Humorous:**\n",
            "\n",
            "* My legs feel like jelly, but my feeling is solid. I'll take victory over exhaustion any day. üòÇ #fitnessfail #MondayMotivation\n",
            "\n",
            "**General:**\n",
            "\n",
            "* Active lifestyles are the best way to live. What is your favorite way to stay active? #wellness #healthyliving \n",
            "* Feeling inspired to trade my couch for miles today. Do you have your workout clothes ready?  üèÉ‚Äç‚ôÄÔ∏è #getoutside #fitnessmotivation\n",
            "\n",
            "\n",
            "**Pro tips for your tweets:**\n",
            "\n",
            "*  Use relevant hashtags  (#fitness, #motivation, #workout, #health) to reach a wider audience. \n",
            "* Include an engaging image or video.\n",
            "* Ask a question to spark conversation. \n",
            "* Keep it short and sweet! \n",
            "\n",
            "\n",
            "Let me know if you want more options! üòä   \n",
            "\n",
            "## Feeling the Strength in Every Step üö∂‚Äç‚ôÄÔ∏èüí™\n",
            "\n",
            "It's been inspiring to see so many of us embrace fitness goals this year! üí™ From pushing our limits in the gym to exploring new outdoor activities, it's truly empowering to celebrate our progress and best of all, learn from one another on this incredible journey. üôå\n",
            "\n",
            "Let's remember that fitness isn't about perfection, it's about setting achievable goals and celebrating each milestone. üéâ  Whether you're joining a new workout program, trying a new healthy recipe, or simply walking for longer after dinner, every step counts towards a healthier, happier you. \n",
            "\n",
            "Who's here on a fitness journey? Share your favourite tips, challenges, or wins in the comments below! üëá \n",
            "\n",
            "#fitness #health #wellness #strength #motivation #personaltraining #healthylifestyle #fitnessjourney #fitnessgoals #community #inspire \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "**Tips for making your post stand out:**\n",
            "\n",
            "* **Relevant post:** Link the post to a personal experience, a recent achievement, or a motivational quote.\n",
            "* **Image:** Include an image or video that represents fitness or positive wellbeing. \n",
            "* **Engagement:** Ask a question or invite people to share their thoughts and experiences. \n",
            "* **Actionable:**  Encourage people to take action, like joining a community group or finding a personal trainer.  \n",
            "* **Hashtags:**  Use relevant and trending hashtags. \n",
            "* **Engagement:** Respond to comments and create further dialogue in the comments. \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You define two prompt templates (one for a tweet, one for a LinkedIn post), connect each prompt to the same LLM and string parser, then run both chains in parallel with the same input topic (\"Fitness\"). The result is a dictionary with two generated texts: one for the tweet and one for the LinkedIn post."
      ],
      "metadata": {
        "id": "55OEW5COUyP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RunnableParallel executes multiple runnables concurrently,\n",
        "When the RunnableParallel is invoked the same input is passed to each child runnable and Each child runnable returns a result.\n",
        "\n",
        "- parallel_chain.invoke({'topic':'Fitness'}) runs the pipeline synchronously and returns the aggregated outputs.\n",
        "Both tweet_chain and linkedin_chain receive {\"topic\": \"Fitness\"}, build their respective prompts, call the LLM, parse the outputs, and return them inside result.\n"
      ],
      "metadata": {
        "id": "WqoU6-Z-VFHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_oetDpLbnfor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "c2zjQ8wrRWjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Runnable Passthrough & Runnable Lambda**"
      ],
      "metadata": {
        "id": "HloLFGhzbeQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain runnable primitives\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        "    RunnableLambda,\n",
        ")\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Hugging Face endpoint + chat wrapper\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "\n",
        "# create HF endpoint LLM (adjust repo_id & task if needed)\n",
        "llm_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=(\"your api key\"),\n",
        ")\n",
        "\n",
        "# chat wrapper around the endpoint\n",
        "model = ChatHuggingFace(llm=llm_endpoint)\n",
        "\n",
        "# prompts and parser\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"Write a joke about {topic}\",\n",
        "    input_variables=[\"topic\"],\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"Explain the following joke - {text}\",\n",
        "    input_variables=[\"text\"],\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# 1) joke generation chain: prompt1 -> model -> parser (produces a plain string)\n",
        "joke_gen_chain = prompt1 | model | parser\n",
        "\n",
        "# 2) small mapper runnable: string joke -> {\"text\": joke}\n",
        "to_text_dict = RunnableLambda(lambda joke: {\"text\": joke})\n",
        "\n",
        "# 3) explanation chain: prompt2 -> model -> parser (expects {\"text\": ...})\n",
        "explain_chain = prompt2 | model | parser\n",
        "\n",
        "# 4) parallel: keep joke as-is, and run explanation by first mapping to {\"text\": joke}\n",
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "        \"joke\": RunnablePassthrough(),               # returns the raw joke string\n",
        "        \"explanation\": to_text_dict | explain_chain, # converts joke -> {\"text\": joke} then explains\n",
        "    }\n",
        ")\n",
        "\n",
        "# 5) final pipeline: generate joke, then run the parallel stage (so both values returned)\n",
        "final_chain = joke_gen_chain | parallel_chain\n",
        "\n",
        "# invoke with the single input required by the first stage\n",
        "result = final_chain.invoke({\"topic\": \"cricket\"})\n",
        "print(result)  # -> {\"joke\": \"....\", \"explanation\": \"....\"}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsiXogUXpm2G",
        "outputId": "c721fe2d-ef3f-4ad9-d97e-6641d48cf08d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'joke': \"Why did the cricket blush? \\n\\nHe saw the ball coming in at the crease! üòÜ üèè \\n\\n\\nLet me know if you'd like to hear another! üèè üòä \\n\", 'explanation': 'This is a classic dad-joke-style silly play on words! Here\\'s the breakdown:\\n\\n* **The setup:**  The joke relies on our expectation of a typical cricket joke about a cricket\\'s action. \\n* **The punchline:**  The key lies in the word \"crease.\" In cricket, the crease is where the batsman stands to face the pitch where the ball is bowled.\\n* **The pun:** \"Crease\" sounds like a synonym for \"blush,\" making a funny connection between the physical action of blushing due to seeing something (a ball) and a context (the crease meant as a baseball metaphor).\\n\\nüòÇ  It\\'s a light-hearted play on words, humorous because it\\'s unexpected and creates a ridiculous juxtaposition. \\n\\n\\nWould you like another cricket joke? ü§î  I\\'ve got a few more up my sleeve! üòäüèè \\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- joke_gen_chain = prompt1 | model | parser <br>\n",
        "prompt1 formats the prompt, <br>\n",
        "model generates text,<br>\n",
        "parser converts the result to str. <br>\n",
        "- RunnableLambda(lambda joke: {\"text\": joke}) wraps a Python lambda as a Runnable and converts a raw joke string into the dictionary {\"text\": joke} which matches prompt2's expected input variable name.\n",
        "- explain_chain = prompt2 | model | parser <br> is similar to joke_gen_chain, but it expects an input dict containing text (the joke).\n",
        "\n",
        "- parallel_chain = RunnableParallel() takes a mapping of keys to runnables and runs them \"in parallel\" over the same input.\n",
        "\n",
        "  - RunnablePassthrough() returns that same joke string\n",
        "\n",
        "  - The 'explanation' pipeline first transforms the string into {\"text\": joke} and then runs the explanation chain which produces the explanation string.\n",
        "\n",
        "  - The output from parallel_chain is a dictionary: {\"joke\": <string>, \"explanation\": <string>}.\n",
        "\n",
        "- final_chain = joke_gen_chain | parallel_chain <br>\n",
        "joke_gen_chain takes {\"topic\": \"cricket\"}  returns the joke string. <br>\n",
        "That joke string becomes the input to parallel_chain. <br>\n",
        "parallel_chain returns {\"joke\": joke_string, \"explanation\": explanation_string}. <br>\n",
        "So final_chain.invoke({\"topic\":\"cricket\"}) returns the final dict in one call.\n",
        "\n"
      ],
      "metadata": {
        "id": "wyDdBFJMYtUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FLOW\n",
        "\n",
        "1. Caller: {\"topic\": \"cricket\"} (dict)\n",
        "\n",
        "2. joke_gen_chain:\n",
        "\n",
        "   - Input: dict {\"topic\": \"cricket\"}\n",
        "\n",
        "   - Output: joke_string (type str)\n",
        "\n",
        "3. parallel_chain receives joke_string and runs both children:\n",
        "\n",
        "   - RunnablePassthrough()  returns joke_string\n",
        "\n",
        "   - to_text_dict (RunnableLambda) receives joke_string returns {\"text\": joke_string}\n",
        "\n",
        "   - explain_chain receives {\"text\": joke_string} returns explanation_string (type str)\n",
        "\n",
        "4. parallel_chain aggregates outputs {\"joke\": joke_string, \"explanation\": explanation_string}"
      ],
      "metadata": {
        "id": "QKRUgYd1adGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "24Ru3OXIq5ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7QdYI-nyq6K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Runnable Branch**"
      ],
      "metadata": {
        "id": "G_8ghurubJCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modern runnable primitives\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        "    RunnableBranch,\n",
        "    RunnableLambda,\n",
        ")\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Hugging Face endpoint + chat wrapper\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "\n",
        "\n",
        "# create HF endpoint LLM (adjust repo_id & task if you want another model)\n",
        "llm_endpoint = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",    # change if you prefer another HF model\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=(\"your api key\"))\n",
        "\n",
        "model = ChatHuggingFace(llm=llm_endpoint)\n",
        "\n",
        "# prompts and parser\n",
        "prompt1 = PromptTemplate(\n",
        "    template=\"Write a detailed report on {topic}\",\n",
        "    input_variables=[\"topic\"],\n",
        ")\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=\"Summarize the following text\\n\\n{text}\",\n",
        "    input_variables=[\"text\"],\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# 1) report generation chain: prompt1 -> model -> parser  (produces string)\n",
        "report_gen_chain = prompt1 | model | parser\n",
        "\n",
        "# 2) summary chain: prompt2 -> model -> parser (expects {\"text\": ...})\n",
        "summary_chain = prompt2 | model | parser\n",
        "\n",
        "# 3) branch: if report is long (>300 words) -> run summary_chain, else passthrough (return report unchanged)\n",
        "#    Note: condition receives the *report string* from report_gen_chain\n",
        "\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x: len(x.split()) > 300, summary_chain),  # if True -> summarize\n",
        "    RunnablePassthrough(),                             # else -> return original report\n",
        ")\n",
        "\n",
        "# 4) parallel: produce both 'report' and 'summary' keys for final output\n",
        "parallel_chain = RunnableParallel(\n",
        "    {\n",
        "        \"report\": RunnablePassthrough(),   # returns the raw report string\n",
        "        \"summary\": branch_chain,           # either a summary string or the original report\n",
        "    }\n",
        ")\n",
        "\n",
        "# 5) final pipeline: generate report, then run the parallel stage to return both values\n",
        "final_chain = report_gen_chain | parallel_chain\n",
        "\n",
        "# invoke with the input required by the first stage\n",
        "result = final_chain.invoke({\"topic\": \"Russia vs Ukraine\"})\n",
        "\n",
        "# result is a dict: {\"report\": \"<full report text>\", \"summary\": \"<summary or original report>\"}\n",
        "print(\"=== Report ===\\n\", result[\"report\"][:1800], \"...\")   # print first 1000 chars to keep colab tidy\n",
        "print(\"\\n === Summary / Fallback === \\n\", result[\"summary\"][:1800], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYG5hFyIp6It",
        "outputId": "3d975de2-5586-4420-c2f8-dabb5650272f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Report ===\n",
            " ##  Russia-Ukraine Conflict: A Detailed Report\n",
            "\n",
            "The ongoing conflict between Russia and Ukraine is a multifaceted crisis with profound geopolitical, humanitarian, and socio-economic consequences. This report provides a detailed examination of various aspects of this rapidly evolving situation:\n",
            "\n",
            "**I. Historical Context and Background:**\n",
            "\n",
            "* **Shared History:** Russia and Ukraine share centuries of intertwined history, with both nations tracing their roots back to the Kyivan Rus' civilization. This shared history has shaped cultural ties and ideological perceptions. \n",
            "* **Soviet Era:**  Ukraine was part of the Soviet Union until 1991, when it declared its independence. Following this, relations between the two nations were characterized by complex interactions, including economic dependence, political tensions, and cultural differences.\n",
            "* **Post-Soviet Period:** Despite attempts at democratic transition, Ukraine sought closer ties with the West while facing challenges from Russia, leading to political and economic friction.\n",
            "* **Euromaidan Revolution and the Annexation of Crimea:** In 2014, protests demanding closer ties with Europe led to the ousting of pro-Russian president Viktor Yanukovych. Russia responded by annexing the Crimean Peninsula, a region with a majority Russian-speaking population, and supporting separatists in eastern Ukraine.\n",
            "\n",
            "**II. Drivers of the Conflict:**\n",
            "\n",
            "* **NATO and Ukraine's EU Aspirations:** Russia sees the expansion of NATO, a Western military alliance, as a threat to its national security. Ukraine's aspirations to join both NATO and the EU have compounded this concern for Russia.\n",
            "* **Geopolitical Influence:** Russia views Ukraine and other nations in Eastern Europe as crucial to maintaining its sphere of influence. Compromising on this perceived ...\n",
            "\n",
            " === Summary / Fallback === \n",
            " The Russia-Ukraine conflict is a multifaceted crisis with devastating consequences. \n",
            "\n",
            "**Historical Context**: Russia and Ukraine share a complex history, intertwined through centuries due to shared cultural roots and the legacy of the Soviet Union. \n",
            "\n",
            "**Drivers**:\n",
            "* Russia views NATO expansion as a threat and intends to maintain control over its buffer zones. \n",
            "* It prioritizes itself as a regional power with influence over Eastern Europe.\n",
            "* Political and ideological differences between Russia and Ukraine contribute to the dispute.\n",
            "\n",
            " **Current Situation**:  In February 2022, Russia launched a full-scale invasion of Ukraine, leading to widespread warfare, a humanitarian crisis, and global economic repercussions.  Ukraine has bravely resisted with fierce guerilla tactics.\n",
            "\n",
            "**Key Impacts**: \n",
            "* **Humanitarian Crisis**: Millions have been displaced both within Ukraine and abroad.\n",
            "* **Geopolitical Spat**: Tensions between Russia and the West have escalated, with increased military presence and efforts for peace.\n",
            "\n",
            "**Future Outlook**: A peaceful resolution remains uncertain. Resolving the crisis will involve confronting underlying political and security issues while prioritizing humanitarian aid and reconstruction.\n",
            "\n",
            "\n",
            "Ultimately, achieving lasting peace and stability depended on ongoing international dialogue, pressure on both sides, and commitment to post-war recovery efforts. \n",
            " ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- prompt1 will create a prompt like \"Write a detailed report on Russia vs Ukraine\" when given {\"topic\": \"Russia vs Ukraine\"}.\n",
        "- prompt2 expects a text variable (the report)\n",
        "\n",
        "- report_gen_chain = prompt1 | model | parser\n",
        "Formats the prompt from {\"topic\": ' '}  a string prompt, <br>\n",
        "Calls the LLM to generate a report, <br>\n",
        "Parses to a string. <br>\n",
        "Input type: {\"topic\": str} Output type: str (the full report).\n",
        "\n",
        "- summary_chain = prompt2 | model | parser <br>\n",
        "expects an input dict {\"text\": <report string>} and returns a summary str.\n",
        "\n",
        "- branch_chain = RunnableBranch() <br>\n",
        "RunnableBranch takes branching rules. <br>\n",
        "\n",
        "  - A tuple (predicate, runnable_if_true): predicate is lambda x: len(x.split()) > 300 <br>\n",
        "  x.split() splits on whitespace, len(...) > 300 checks if the report has more than 300 words <br>\n",
        "  If True summary_chain is executed.\n",
        "\n",
        "  - A second argument is the runnable to run if predicate is false: RunnablePassthrough().\n",
        "\n",
        "- parallel_chain = RunnableParallel() <br>\n",
        "runs both children with the same input <br>\n",
        "\"report\" uses RunnablePassthrough() so that the final output includes the full report unchanged. <br>\n",
        "\"summary\" runs the branch. The branch returns either:\n",
        "a summary string (if report > 300 words), or <br>\n",
        "the original report (via passthrough) if it is short.\n",
        "\n",
        "- final_chain = report_gen_chain | parallel_chain <br>\n",
        "full flow: <br>\n",
        "report_gen_chain (input: {\"topic\": ...} -> output: report_str) <br>\n",
        "parallel_chain (input: report_str -> output: {\"report\": report_str, \"summary\": summary_or_report}) <br>\n",
        "Final invoke returns the dict described.\n",
        "\n",
        "# Flow\n",
        "\n",
        "1. Caller passes {\"topic\": str} to final_chain. <br>\n",
        "2. report_gen_chain returns report_str (type str) <br>\n",
        "3. parallel_chain gets report_str and runs: <br>\n",
        " \"report\" child returns report_str (passthrough). <br>\n",
        " \"summary\" child ‚Üí runs branch_chain:\n",
        "   - If True: run summary_chain (which expects {\"text\": report_str}) and returns summary string. <br>\n",
        "   - If False: RunnablePassthrough() returns report_str. <br>\n",
        "4. parallel_chain returns {\"report\": report_str, \"summary\": ...}"
      ],
      "metadata": {
        "id": "JKtS-j8blPCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zi8BVayCoP19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ks__r4VBrqko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PromptTemplate for structured prompt construction\n",
        "\n",
        "2. HuggingFaceEndpoint + ChatHuggingFace to run LLM models\n",
        "\n",
        "3. StrOutputParser to clean and normalize model output\n",
        "\n",
        "4. RunnableSequence (| operator) to chain steps (prompt -> model -> parser)\n",
        "\n",
        "5. RunnableParallel for multi-branch outputs (tweet + LinkedIn, report + summary)\n",
        "\n",
        "6. RunnableLambda for lightweight transformations (string -> dict mappings)\n",
        "\n",
        "7. RunnableBranch for conditional logic (summarize only if long)\n",
        "\n",
        "8. RunnablePassthrough for retaining upstream outputs without modification"
      ],
      "metadata": {
        "id": "JP20FAgVpSvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AV9DcEIApqD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ensure Input‚ÄìOutput Shape Consistency <br>\n",
        "Always confirm that downstream runnables (eg summarizers) receive the structured input they expect. Use RunnableLambda for safe transformations.\n",
        "\n",
        "2. Prefer JSON-Structured Outputs <br>\n",
        "When possible, request JSON and parse with structured output parsers for reliability and downstream automation.\n",
        "\n",
        "3. Use ainvoke() + asyncio when calling multiple models or long-generation pipelines to reduce wall-time.\n",
        "\n",
        "4. Secure API Keys  <br>\n",
        "Use environment variables or secret managers‚Äîavoid hardcoding tokens in pipeline code."
      ],
      "metadata": {
        "id": "ipXK988gpq2-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCtxbycUrXRK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}