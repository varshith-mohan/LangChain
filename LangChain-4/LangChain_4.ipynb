{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "hOHGMRFMDGlY",
        "outputId": "20995582-24eb-45a3-924c-e74158757b3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
            "Collecting langchain-core<2.0.0,>=1.0.3 (from langchain_huggingface)\n",
            "  Downloading langchain_core-1.0.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.4.40)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (2.11.10)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.10.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_core-1.0.3-py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain_huggingface\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.0.3 langchain_huggingface-1.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langchain_core"
                ]
              },
              "id": "d6f289eaec9a4e1aa71aebd80611c984"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Chain**"
      ],
      "metadata": {
        "id": "7LRzSa-DG7lS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sgwjXWptvV2",
        "outputId": "cf09ade2-afbc-48b7-8203-5bb04ff23f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 interesting facts about nutrition:\n",
            "\n",
            "1. **Your gut microbiome is crucial for your overall health.**  The trillions of bacteria living in your gut play a role in digestion, immunity, and even brain function!  A healthy gut microbiome is linked to better nutrient absorption, reduced illness risk, and potentially positive mood. \n",
            "2. **Food label deciphering is more nuanced than it seems.**  The nutritional facts label tells you a lot about the food, but remember: serving sizes can differ drastically.  Don't worry, it helps to read the **percentage daily value** (DV) for specific nutrients – how much you're getting compared to a typical diet.\n",
            "3. **Sleep and nutrition are interconnected.** The right diet and sleep quality in tandem with healthy routines can enhance both.  Studies show that even mild sleep deprivation can increase cravings for high-sugar and high-fat foods. Getting enough sleep helps regulate hormones that control hunger and fullness.\n",
            "4. **What you eat doesn't always stay where you eat.**  Your body digests food for energy, and small amounts can even become part of your stool and waste away! This is where fiber comes in; it keeps your digestive system working efficiently, which can help your body get rid of harmful substances through urine and feces.\n",
            "5. **Water isn't just for hydration.**  Water regulates body temperature and helps transport nutrients from your digestive system to your cells. Lack of water can lead to fatigue and headaches, while sufficient hydration supports healthy cell function and cognitive performance.\n",
            "\n",
            "I hope these facts are surprise tasty! \n",
            "\n",
            "\n",
            "Let me know if you'd like to dive deeper into any specific fact, or if you've got another nutrition question you'd like to explore. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id='google/gemma-2-2b-it', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template='Generate 5 interesting facts about {topic}',\n",
        "    input_variables=['topic']\n",
        ")\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "result = chain.invoke({'topic':'Nutrition'})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_text = result\n",
        "\n",
        "# Use the replace method to swap all occurrences of '*' with an empty string ''\n",
        "cleaned_text = original_text.replace('*', '')\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxciC8pUFSE2",
        "outputId": "98c0b509-8202-4abb-a2ab-ef4e057c544a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 interesting facts about nutrition:\n",
            "\n",
            "1. Your gut microbiome is crucial for your overall health.  The trillions of bacteria living in your gut play a role in digestion, immunity, and even brain function!  A healthy gut microbiome is linked to better nutrient absorption, reduced illness risk, and potentially positive mood. \n",
            "2. Food label deciphering is more nuanced than it seems.  The nutritional facts label tells you a lot about the food, but remember: serving sizes can differ drastically.  Don't worry, it helps to read the percentage daily value (DV) for specific nutrients – how much you're getting compared to a typical diet.\n",
            "3. Sleep and nutrition are interconnected. The right diet and sleep quality in tandem with healthy routines can enhance both.  Studies show that even mild sleep deprivation can increase cravings for high-sugar and high-fat foods. Getting enough sleep helps regulate hormones that control hunger and fullness.\n",
            "4. What you eat doesn't always stay where you eat.  Your body digests food for energy, and small amounts can even become part of your stool and waste away! This is where fiber comes in; it keeps your digestive system working efficiently, which can help your body get rid of harmful substances through urine and feces.\n",
            "5. Water isn't just for hydration.  Water regulates body temperature and helps transport nutrients from your digestive system to your cells. Lack of water can lead to fatigue and headaches, while sufficient hydration supports healthy cell function and cognitive performance.\n",
            "\n",
            "I hope these facts are surprise tasty! \n",
            "\n",
            "\n",
            "Let me know if you'd like to dive deeper into any specific fact, or if you've got another nutrition question you'd like to explore. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grandalf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQOggg3bDki0",
        "outputId": "9479953f-522a-4937-d314-aeaf7ca11277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grandalf in /usr/local/lib/python3.12/dist-packages (0.8)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from grandalf) (3.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Chain = chain.get_graph().draw_mermaid()\n",
        "print(Chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItWHBo7ND_EF",
        "outputId": "d9178a22-39de-4bd1-f89d-646389c0fd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\tPromptInput([PromptInput]):::first\n",
            "\tPromptTemplate(PromptTemplate)\n",
            "\tChatHuggingFace(ChatHuggingFace)\n",
            "\tStrOutputParser(StrOutputParser)\n",
            "\tStrOutputParserOutput([StrOutputParserOutput]):::last\n",
            "\tPromptInput --> PromptTemplate;\n",
            "\tPromptTemplate --> ChatHuggingFace;\n",
            "\tStrOutputParser --> StrOutputParserOutput;\n",
            "\tChatHuggingFace --> StrOutputParser;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "20cGj-D9FvOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sequential Chain**"
      ],
      "metadata": {
        "id": "dzLV7HdMG16H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(repo_id='google/gemma-2-2b-it', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "prompt1 = PromptTemplate(template='Generate a detailed report on {topic}', input_variables=['topic'])\n",
        "\n",
        "prompt2 = PromptTemplate(template='Generate a 5 pointer summary from the following text \\n {text}', input_variables=['text'])\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt1 | model | parser | prompt2 | model | parser\n",
        "\n",
        "result = chain.invoke({'topic':'Fitness'})\n",
        "\n",
        "cleaned_text = result.replace('*', '')\n",
        "\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ew1f1TTEWbJ",
        "outputId": "2c0b23ac-6b50-4a7d-a58a-d841c30aae4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Fitness: 5 Pointer Summary \n",
            "\n",
            "1. Definition:  Fitness is a multifaceted concept encompassing physical, mental, and emotional well-being, promoting optimal function, performance, and vitality.\n",
            "2. Components: Fitness is made up of five core components: cardiovascular endurance, muscular strength, muscular endurance, flexibility, and body composition. \n",
            "3. Benefits: Maintaining fitness brings numerous benefits, including improved cardiovascular health, weight management, enhanced bone density, improved mood, better sleep, and increased energy levels. Additionally, it contributes to cognitive function, reduced anxiety, and increased confidence.\n",
            "4. Influencing Factors: Individuals' fitness levels are affected by age, fitness goals, diet, motivation, genetic makeup, overall health, and social support systems.\n",
            "5. Recommendations:  For optimal results, consult a healthcare professional, start slowly and gradually increase intensity, listen to your body's signals, focus on consistent activity, incorporate enjoyment activities, and embrace stress-management practices for a fulfilling and healthy life. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Chain = chain.get_graph().draw_mermaid()\n",
        "print(Chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPtRCHWjGbcv",
        "outputId": "0108061d-7fa3-478f-b155-8922e71781f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\tPromptInput([PromptInput]):::first\n",
            "\tPromptTemplate_1(PromptTemplate)\n",
            "\tChatHuggingFace_1(ChatHuggingFace)\n",
            "\tStrOutputParser_1(StrOutputParser)\n",
            "\tStrOutputParserOutput_1(StrOutputParserOutput)\n",
            "\tPromptTemplate_2(PromptTemplate)\n",
            "\tChatHuggingFace_2(ChatHuggingFace)\n",
            "\tStrOutputParser_2(StrOutputParser)\n",
            "\tStrOutputParserOutput_2([StrOutputParserOutput]):::last\n",
            "\tPromptInput --> PromptTemplate_1;\n",
            "\tPromptTemplate_1 --> ChatHuggingFace_1;\n",
            "\tStrOutputParser_1 --> StrOutputParserOutput_1;\n",
            "\tChatHuggingFace_1 --> StrOutputParser_1;\n",
            "\tStrOutputParserOutput_1 --> PromptTemplate_2;\n",
            "\tPromptTemplate_2 --> ChatHuggingFace_2;\n",
            "\tStrOutputParser_2 --> StrOutputParserOutput_2;\n",
            "\tChatHuggingFace_2 --> StrOutputParser_2;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Vf0hw_J6G_hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parallel Chain**"
      ],
      "metadata": {
        "id": "kGSi351hYGMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "\n",
        "llm1 = HuggingFaceEndpoint(repo_id='google/gemma-2-2b-it', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model1 = ChatHuggingFace(llm=llm1)\n",
        "\n",
        "\n",
        "llm2 = HuggingFaceEndpoint(repo_id='MiniMaxAI/MiniMax-M2', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model2 = ChatHuggingFace(llm=llm2)\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Generate brief notes from the following text \\n {text}',\n",
        "    input_variables=['text'])\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Generate 6 short question and answers from the following text \\n {text}',\n",
        "    input_variables=['text'])\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Merge the provided notes and quiz into a single document \\n notes -> {notes} and quiz -> {quiz}',\n",
        "    input_variables=['notes', 'quiz'])\n",
        "\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "parallel_chain = RunnableParallel({ 'notes': prompt1 | model1 | parser, 'quiz': prompt2 | model2 | parser })\n",
        "\n",
        "\n",
        "merged_chain = prompt3 | model1 | parser\n",
        "\n",
        "\n",
        "chain = parallel_chain | merged_chain\n",
        "\n",
        "text = \"\"\"\n",
        "Support vector machines (SVMs) and Decision Tree are a set of supervised learning methods used for classification, regression and outliers detection.\n",
        "\n",
        "The advantages of support vector machines are:\n",
        "\n",
        "Effective in high dimensional spaces.\n",
        "\n",
        "Still effective in cases where number of dimensions is greater than the number of samples.\n",
        "\n",
        "Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
        "\n",
        "Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
        "\n",
        "The disadvantages of support vector machines include:\n",
        "\n",
        "If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
        "\n",
        "SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
        "\n",
        "The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\n",
        "\"\"\"\n",
        "\n",
        "result = chain.invoke({'text':text})\n",
        "\n",
        "\n",
        "cleaned_text = result.replace('*', '')\n",
        "\n",
        "print(cleaned_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "8pibOgOMGx5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175961b1-41a3-4c22-f65b-630651594c79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## 6 Q&A about Support Vector Machines (SVMs)\n",
            "\n",
            "1. Q: What tasks can SVMs and Decision Trees be used for?  \n",
            "A: SVMs are used for a variety of supervised learning tasks including classification, regression, and outlier detection. Decision Trees are also used for all these tasks.\n",
            "\n",
            "2. Q: In what situations are SVMs especially effective?\n",
            "A: They remain effective even when the number of dimensions exceeds the number of samples, meaning they can handle high-dimensional data. \n",
            "\n",
            "3. Q: Why are SVMs memory-efficient?  \n",
            "A: They are memory-efficient because their decision function depends only on a subset of training points (support vectors), not the whole dataset.\n",
            "\n",
            "4. Q: How can SVMs be adapted to different problems?  \n",
            "A: SVMs can be tailored to different problems by specifying different kernel functions. You can use common kernels already provided or customize your own. \n",
            "\n",
            "5. Q:  What caution is needed when features far outnumber samples, and what input formats work best?\n",
            "A: Carefully select kernel functions and regularization to avoid overfitting, as with a large number of features, SVMs can become more susceptible to overfitting. For prediction, use dense (preferably C-ordered `numpy.ndarray`) or sparse (preferably `scipy.sparse.csr_matrix`) vectors with `float64` type. The SVMs must have been fitted on such sparse data before predicting on data.\n",
            "\n",
            "6. Q: How are probability estimates obtained from a scikit-learn SVM? \n",
            "A: Probability estimates are not directly provided; they are obtained using an expensive five-fold cross-validation on the trained model.  \n",
            "\n",
            "\n",
            "\n",
            "Let me know if you want to explore other questions or find answers about other specific aspects of SVMs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- RunnableParallel allows you to execute tasks in parallel. You can chain multiple tasks to run simultaneously, which improves efficiency.\n",
        "\n",
        "- RunnableParallel({ 'notes': prompt1 | model1 | parser, 'quiz': prompt2 | model2 | parser }) <br>\n",
        "Here, two tasks are defined: <br>\n",
        "Notes Generation: prompt1 -> model1 -> parser <br>\n",
        "Quiz Generation: prompt2 -> model2 -> parser <br>\n",
        "The pipe operator (|) is used to chain the components together: prompt1 generates the input for the model. model1 generates output based on the input. parser cleans up the output. The parallel_chain will run these two tasks (notes and quiz) simultaneously.\n",
        "\n",
        "- prompt3 merges the notes (notes) and quiz (quiz) into a single document.\n",
        "The merged output is then processed by model1 and cleaned by parser.\n",
        "\n",
        "- final chain is created by combining parallel_chain and merged_chain. Parallel Execution happens first: notes and quiz are generated at the same time, After that, Merging happens: the notes and quiz are combined into a final document."
      ],
      "metadata": {
        "id": "DpLttt2BwiF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Notes Generation - first language model generates brief notes based on the input text.\n",
        "\n",
        "2. Quiz Generation - second language model generates short questions and answers based on the input text.\n",
        "\n",
        "3. Parallel Execution - Both notes and quiz are generated in parallel.\n",
        "\n",
        "4. Merging - The generated notes and quiz are merged into a single document.\n",
        "\n",
        "5. Output Cleaning - The final document is cleaned up to remove unwanted characters.\n",
        "\n",
        "6. Final Output - The cleaned output is printed."
      ],
      "metadata": {
        "id": "6gXgUhBZyHnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qgkAmQUJhRwz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HH-pM0I1YXrK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conditional Chain**"
      ],
      "metadata": {
        "id": "7ayO4PWbwY3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "llm2 = HuggingFaceEndpoint(repo_id='meta-llama/Llama-3.1-8B-Instruct', task='text-generation', huggingfacehub_api_token='your api key')\n",
        "\n",
        "model2 = ChatHuggingFace(llm=llm2)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "\n",
        "class Feedback(BaseModel):\n",
        "\n",
        "    sentiment: Literal['positive', 'negative'] = Field(description='Give the sentiment of the feedback')\n",
        "\n",
        "parser2 = PydanticOutputParser(pydantic_object=Feedback)\n",
        "\n",
        "\n",
        "\n",
        "prompt1 = PromptTemplate(\n",
        "    template='Classify the sentiment of the following feedback text into postive or negative \\n {feedback} \\n {format_instruction}',\n",
        "    input_variables=['feedback'],\n",
        "    partial_variables={'format_instruction':parser2.get_format_instructions()}\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "classifier_chain = prompt1 | model2 | parser2\n",
        "\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template='Write an appropriate response to this positive feedback \\n {feedback}',\n",
        "    input_variables=['feedback'])\n",
        "\n",
        "\n",
        "prompt3 = PromptTemplate(\n",
        "    template='Write an appropriate response to this negative feedback \\n {feedback}',\n",
        "    input_variables=['feedback'])\n",
        "\n",
        "\n",
        "# cleaner function to remove Markdown stars\n",
        "def clean_output(text: str) -> str:\n",
        "    return text.replace('*', '').strip()\n",
        "\n",
        "# conditional branching based on sentiment\n",
        "branch_chain = RunnableBranch(\n",
        "    (lambda x: x.sentiment == 'positive', prompt2 | model2 | parser | RunnableLambda(clean_output)),\n",
        "    (lambda x: x.sentiment == 'negative', prompt3 | model2 | parser | RunnableLambda(clean_output)),\n",
        "    RunnableLambda(lambda x: 'could not find the suitable sentiment'),\n",
        ")\n",
        "\n",
        "# combine both chains\n",
        "chain = classifier_chain | branch_chain\n",
        "\n",
        "# examples\n",
        "print('\\n Positive Example:\\n')\n",
        "print(chain.invoke({'feedback': 'This is a beautiful waterfall'}))\n",
        "\n",
        "print('\\n Negative Example:\\n')\n",
        "print(chain.invoke({'feedback': 'The pizza was terrible'}))\n",
        "\n",
        "print('\\n Neutral Example:\\n')\n",
        "print(chain.invoke({'feedback': 'The weather is fine'}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-cOx_gMY7M-",
        "outputId": "d2e827da-ad6f-4138-bfbc-4c82fd5c49ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Positive Example:\n",
            "\n",
            "Thank you so much for your kind words. I'm thrilled to hear that you enjoyed our interaction and found it helpful. I'm always here to assist and provide the best possible service, so please don't hesitate to reach out if you have any other questions or concerns.\n",
            "\n",
            " Negative Example:\n",
            "\n",
            "Here's a sample response to negative feedback:\n",
            "\n",
            "Response:\n",
            "\n",
            "\"Thank you for taking the time to share your feedback with us. I apologize that we fell short of meeting your expectations. I'd love to understand more about what went wrong and how we can improve. Could you please provide more details about your experience? This will help us to identify areas for improvement and prevent similar issues from happening in the future. Your feedback is invaluable to us and we appreciate your honesty.\"\n",
            "\n",
            "Alternative Response:\n",
            "\n",
            "\"I'm sorry to hear that we didn't meet your expectations. Can you please tell me more about what you were looking for and how we can do better? I'll make sure to pass on your feedback to the relevant team and we'll do our best to make things right.\"\n",
            "\n",
            " Neutral Example:\n",
            "\n",
            "Here are a few responses to positive feedback:\n",
            "\n",
            "1. \"Thank you so much! I'm glad you enjoyed it.\"\n",
            "2. \"I'm thrilled to hear that! Thank you for your kind words.\"\n",
            "3. \"That means a lot to me. I appreciate your feedback.\"\n",
            "4. \"I'm happy to hear that you liked it. Thank you for your support.\"\n",
            "5. \"Thank you for your kind words. I'm glad I could make a positive impact.\"\n",
            "\n",
            "You can choose the one that best fits the situation and the tone you want to convey.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feedback class inherits BaseModel from Pydantic that contains only one field, sentiment, which can only be 'positive' or 'negative'.\n",
        "\n",
        "- This schema will ensure correct model outputs — i.e if the model output does not match this structure, LangChain will raise an error or ask the model to fix the output.\n",
        "\n",
        "- parser2 knows how to parse the LLM output into the Feedback Pydantic object, It ensures that the text output from the LLM matches to the expected structured format ({'sentiment': 'positive'} or {'sentiment': 'negative'})\n",
        "\n"
      ],
      "metadata": {
        "id": "9J_poqrlsH_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- prompt1 creates a template prompt that asks the model to classify a piece of text into positive or negative <br>\n",
        "{feedback} is a placeholder that will be replaced with the actual feedback text <br>\n",
        "{format_instruction} is automatically generated by the parser — it tells the LLM to respond in structured JSON format (like {\"sentiment\": \"positive\"})"
      ],
      "metadata": {
        "id": "1sh4v7h1szeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- classifier_chain -- prompt1 prepares the text prompt <br>\n",
        "model2 generates text output e.g {'sentiment': 'positive'} <br>\n",
        "parser2 parses and validates that output against the Feedback schema."
      ],
      "metadata": {
        "id": "wmYfG3tQtYqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- prompt2: Used if feedback is positive <br>\n",
        "prompt3: Used if feedback is negative <br>\n",
        "Each will generate a relevant response based on the feedback text."
      ],
      "metadata": {
        "id": "9Udzt105trpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RunnableBranch is like an if-else statement for LangChain pipelines.**\n",
        "\n",
        "It takes conditions and corresponding actions: <br>\n",
        "\n",
        "If sentiment is positive:  <br>\n",
        "         Condition: lambda x: x.sentiment == 'positive' <br>\n",
        "         Action: Run prompt2 | model | parser <br>\n",
        "\n",
        "If sentiment is negative: <br>\n",
        "         Condition: lambda x: x.sentiment == 'negative' <br>\n",
        "         Action: Run prompt3 | model | parser <br>\n",
        "\n",
        "Else: <br>\n",
        "      If neither condition matches, it runs a default lambda returning <br>\n",
        "       \"could not find the suitable sentiment\" <br>\n",
        "\n",
        "x here refers to the output from the previous chain — i.e Feedback from classifier_chain"
      ],
      "metadata": {
        "id": "fJhX6vfWt4OI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chain = classifier_chain | branch_chain <br>\n",
        "suns the classifier_chain -> gets a Feedback with the sentiment <br>\n",
        "next feed that result into the branch_chain -> decides which response prompt to use based on sentimennt"
      ],
      "metadata": {
        "id": "-v-DdJaauqhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Chain = chain.get_graph().draw_mermaid()\n",
        "print(Chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM2PE7luszBZ",
        "outputId": "db692f2e-1d6f-4dd8-980c-07885822de46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\tPromptInput([PromptInput]):::first\n",
            "\tPromptTemplate(PromptTemplate)\n",
            "\tChatHuggingFace(ChatHuggingFace)\n",
            "\tPydanticOutputParser(PydanticOutputParser)\n",
            "\tBranch(Branch)\n",
            "\tBranchOutput([BranchOutput]):::last\n",
            "\tPromptInput --> PromptTemplate;\n",
            "\tPromptTemplate --> ChatHuggingFace;\n",
            "\tChatHuggingFace --> PydanticOutputParser;\n",
            "\tBranch --> BranchOutput;\n",
            "\tPydanticOutputParser --> Branch;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClOspfau2CmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}