{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
      "Collecting langchain-core<2.0.0,>=1.0.3 (from langchain_huggingface)\n",
      "  Downloading langchain_core-1.0.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.4.38)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (2.11.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.3->langchain_huggingface) (1.3.1)\n",
      "Downloading langchain_huggingface-1.0.1-py3-none-any.whl (27 kB)\n",
      "Downloading langchain_core-1.0.3-py3-none-any.whl (469 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-core, langchain_huggingface\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.79\n",
      "    Uninstalling langchain-core-0.3.79:\n",
      "      Successfully uninstalled langchain-core-0.3.79\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-1.0.3 langchain_huggingface-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j2xXpb43Y2R"
   },
   "source": [
    "StructuredOutputParser we can define schema which will be followed by the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fact_1': 'The Bermuda Triangle is a loosely defined area in the western part of the North Atlantic ocean, roughly bounded by the southeastern United States, Bermuda, and Puerto Rico.', 'fact_2': 'A common misconception is that ships and aircraft mysteriously disappear within the Triangle.  However, the Bureau of Safety and Environmental Enforcement states that the disappearances within this region are no greater than rates in other areas of the ocean.', 'fact_3': 'The Bermuda Triangle is often associated with ghost stories, UFO sightings, and unexplained events. These myths have captivated the public imagination for decades.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id='google/gemma-2-2b-it', task='text-generation', huggingfacehub_api_token='your api key')\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'Bermuda Triangle'})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8iqJKBDzxgK"
   },
   "source": [
    "- Each ResponseSchema defines a field you want in the output In this case, we are asking for three facts: fact_1,fact_2,fact_3\n",
    "\n",
    "- StructuredOutputParser.from_response_schemas(schema) tells LangChain how to parse the models response into the structured format (JSON with 3 keys: fact_1, fact_2, fact_3). parser.get_format_instructions() automatically generates a text prompt telling the LLM how to structure its response.\n",
    "\n",
    "- template: The text that will be sent to the LLM.\n",
    "{topic}: Will be replaced by the topic you provide.\n",
    "{format_instruction}: Will be replaced by parser instructions (to ensure JSON format output).\n",
    "So the final prompt looks like:\n",
    "Give 3 fact about Bermuda Triangle\n",
    "The output should be in JSON format with keys: fact_1, fact_2, fact_3.\n",
    "\n",
    "- langChain Expression Language (LCEL) pipeline:\n",
    "template - formats the input prompt.\n",
    "model - sends the prompt to the Hugging Face LLM.\n",
    "parser - parses the raw output into structured data (dictionary).\n",
    "\n",
    "- chain.invoke\n",
    "Replaces {topic} with \"Bermuda Triangle\".\n",
    "Sends prompt - receives model response → parses it into JSON.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQbUnTQ_13ub"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okJvA0VK3K9a"
   },
   "source": [
    "StrOutputParser takes output from LLm and return plain string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The United Kingdom: A Summary\n",
      "\n",
      "1. **Powerhouse Nation:** The UK is a global power with a rich history, diverse cultures, and influential role in international affairs, spanning geographically across the British Isles.\n",
      "2. **Past Glory and Innovation:** Its history boasts Roman and Anglo-Saxon influences, with robust periods in medieval and industrial history, emerging as a leader in education and technology.\n",
      "3. **Modern Economy:** A thriving economy driven largely by the service sector, with London as a global financial hub, boasting strengths in fintech and technology.\n",
      "4. **Diverse Demographics and Society:** Featuring diverse populations and multicultural influences, the UK prides itself on  a multi-faith, vibrant society.\n",
      "5. **Challenges and Contentious Future:** Navigating post-Brexit impacts, facing economic uncertainties, and addressing social inequalities are challenges facing the UK moving forward. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token='your api key'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 line summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "prompt1 = template1.invoke({'topic':'United Kingdom'})\n",
    "\n",
    "result = model.invoke(prompt1)\n",
    "\n",
    "prompt2 = template2.invoke({'text':result.content})\n",
    "\n",
    "result1 = model.invoke(prompt2)\n",
    "\n",
    "print(result1.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a7Yu5L95ei5"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqsc-HWi6bXS"
   },
   "source": [
    "PydanticOutputParser converts LLM response into well structured and validated python objects based on defined schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Thomas Sutherland' age=32 city='Perth'\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "# Define the model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\", huggingfacehub_api_token='your api key')\n",
    "\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "class Person(BaseModel):\n",
    "\n",
    "    name:  str = Field(description = 'Name of the person')\n",
    "    age :  int = Field(gt=18, description = 'Age of the person')\n",
    "    city:  str = Field(description = 'Name of the city the person belongs to')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "final_result = chain.invoke({'place':'Australia'})\n",
    "\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLiE3S-Z7ZGy"
   },
   "source": [
    "- creating a structured data model called Person using Pydantic.\n",
    "Each field has:\n",
    "A type (str, int)\n",
    "A description (used by LangChain to instruct the LLM)\n",
    "Optional validation rules (like gt=18 ensures age > 18)\n",
    "So this schema expects:\n",
    "{\n",
    "  \"name\": \"Alice\",\n",
    "  \"age\": 25,\n",
    "  \"city\": \"Paris\"\n",
    "}\n",
    "\n",
    "- PydanticOutputParser(pydantic_object=Person) tells LangChain Whatever the LLM outputs must fit this Person structure\n",
    "PydanticOutputParser automatically generates formatting instructions that tell the model how to respond in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6dahXm38AUm"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0YiaG2c-Zg7"
   },
   "source": [
    "JsonOutputParser converts LLM response into JSON Object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['Rohit Sharma is an Indian cricketer and former captain of the Indian national team.', 'He is renowned for his exceptional batting skills, particularly known for his aggressive strokeplay and ability to hit boundaries.', 'He holds the record for the fastest century in ODI cricket, achieving it in just 35 balls.', 'He is a three-time IPL (Indian Premier League) champion, winning with Mumbai Indians.', 'Named the best batsman in the world for the ICC World XI in 2019.']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "# Define the model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"google/gemma-2-2b-it\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token='your api key'\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'Rohit Sharma'})\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rohit Sharma is an Indian cricketer and former captain of the Indian national team.\n",
      "He is renowned for his exceptional batting skills, particularly known for his aggressive strokeplay and ability to hit boundaries.\n",
      "He holds the record for the fastest century in ODI cricket, achieving it in just 35 balls.\n",
      "He is a three-time IPL (Indian Premier League) champion, winning with Mumbai Indians.\n",
      "Named the best batsman in the world for the ICC World XI in 2019.\n"
     ]
    }
   ],
   "source": [
    "paragraph = '\\n'.join(result['facts'])\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
